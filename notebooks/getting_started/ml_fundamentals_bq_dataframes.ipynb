{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur8xi4C7S06n"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPoU8Sm5E6e"
      },
      "source": [
        "# Machine Learning Fundamentals with BigQuery DataFrames\n",
        "\n",
        "<table align=\"left\">\n",
        "\n",
        "  <td>\n",
        "    <a href=\"https://colab.research.google.com/github/googleapis/python-bigquery-dataframes/blob/main/notebooks/getting_started/ml_fundamentals_bq_dataframes.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://github.com/googleapis/python-bigquery-dataframes/blob/main/notebooks/getting_started/ml_fundamentals_bq_dataframes.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
        "      View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/googleapis/python-bigquery-dataframes/blob/main/notebooks/getting_started/ml_fundamentals_bq_dataframes.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
        "      Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>                                                                                               \n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24743cf4a1e1"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvgnzT1CKxrO"
      },
      "source": [
        "## Overview\n",
        "\n",
        "The `bigframes.ml` module implements Scikit-Learn's machine learning API in\n",
        "BigQuery DataFrames. It exposes BigQuery's ML capabilities in a simple, popular\n",
        "API that works seamlessly with the rest of the BigQuery DataFrames API.\n",
        "\n",
        "Learn more about [BigQuery DataFrames](https://cloud.google.com/python/docs/reference/bigframes/latest)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d975e698c9a4"
      },
      "source": [
        "### Objective\n",
        "\n",
        "In this tutorial, you will walk through an end-to-end machine learning workflow using BigQuery DataFrames. You will load data, manipulate and prepare it for model training, build supervised and unsupervised models, and evaluate and save a model for future use; all using built-in BigQuery DataFrames functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08d289fa873f"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This tutorial uses the [```penguins``` table](https://console.cloud.google.com/bigquery?p=bigquery-public-data&d=ml_datasets&t=penguins) (a BigQuery public dataset), which contains data on a set of penguins including species, island of residence, weight, culmen length and depth, flipper length, and sex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aed92deeb4a0"
      },
      "source": [
        "### Costs\n",
        "\n",
        "This tutorial uses billable components of Google Cloud:\n",
        "\n",
        "* BigQuery (storage and compute)\n",
        "* BigQuery ML\n",
        "\n",
        "Learn about [BigQuery storage pricing](https://cloud.google.com/bigquery/pricing#storage),\n",
        "[BigQuery compute pricing](https://cloud.google.com/bigquery/pricing#analysis_pricing_models),\n",
        "and [BigQuery ML pricing](https://cloud.google.com/bigquery/pricing#bqml),\n",
        "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
        "to generate a cost estimate based on your projected usage."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7EUnXsZhAGF"
      },
      "source": [
        "## Installation\n",
        "\n",
        "Depending on your Jupyter environment, you might have to install packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRTcBQPZpKWd"
      },
      "source": [
        "**Vertex AI Workbench or Colab**\n",
        "\n",
        "Do nothing, BigQuery DataFrames package is already installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdOJtFo1pRnc"
      },
      "source": [
        "**Local JupyterLab instance**\n",
        "\n",
        "Uncomment and run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfPoOwPLGpSr"
      },
      "outputs": [],
      "source": [
        "# !pip install bigframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF1j6f9HApxa"
      },
      "source": [
        "## Before you begin\n",
        "\n",
        "Complete the tasks in this section to set up your environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq7zKYWelRQP"
      },
      "source": [
        "### Set up your Google Cloud project\n",
        "\n",
        "**The following steps are required, regardless of your notebook environment.**\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 credit towards your compute/storage costs.\n",
        "\n",
        "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "\n",
        "3. [Click here](https://console.cloud.google.com/flows/enableapi?apiid=bigquery.googleapis.com) to enable the BigQuery API.\n",
        "\n",
        "4. If you are running this notebook locally, install the [Cloud SDK](https://cloud.google.com/sdk)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WReHDGG5g0XY"
      },
      "source": [
        "#### Set your project ID\n",
        "\n",
        "If you don't know your project ID, try the following:\n",
        "* Run `gcloud config list`.\n",
        "* Run `gcloud projects list`.\n",
        "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM1iC_MfAts1"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Set the project id\n",
        "! gcloud config set project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "region"
      },
      "source": [
        "#### Set the region\n",
        "\n",
        "You can also change the `REGION` variable used by BigQuery. Learn more about [BigQuery regions](https://cloud.google.com/bigquery/docs/locations#supported_locations)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eF-Twtc4XGem"
      },
      "outputs": [],
      "source": [
        "REGION = \"US\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcW9adriUQRc"
      },
      "source": [
        "#### Set the dataset ID\n",
        "\n",
        "As part of this notebook, you will save BigQuery ML models to your Google Cloud project, which requires a dataset. Create the dataset, if needed, and provide the ID here as the `DATASET` variable used by BigQuery. Learn how to create a [BigQuery dataset](https://cloud.google.com/bigquery/docs/datasets)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbMh9JHvUHAn"
      },
      "outputs": [],
      "source": [
        "DATASET = \"\"  # @param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwxfWoR5UGwO"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBCra4QMA2wR"
      },
      "source": [
        "### Authenticate your Google Cloud account\n",
        "\n",
        "Depending on your Jupyter environment, you might have to manually authenticate. Follow the relevant instructions below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74ccc9e52986"
      },
      "source": [
        "**Vertex AI Workbench**\n",
        "\n",
        "Do nothing, you are already authenticated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de775a3773ba"
      },
      "source": [
        "**Local JupyterLab instance**\n",
        "\n",
        "Uncomment and run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "254614fa0c46"
      },
      "outputs": [],
      "source": [
        "# ! gcloud auth login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef21552ccea8"
      },
      "source": [
        "**Colab**\n",
        "\n",
        "Uncomment and run the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "603adbbf0532"
      },
      "outputs": [],
      "source": [
        "# from google.colab import auth\n",
        "# auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "960505627ddf"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQmSRbKA8r-"
      },
      "outputs": [],
      "source": [
        "import bigframes.pandas as bf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "init_aip:mbsdk,all"
      },
      "source": [
        "\n",
        "### Set BigQuery DataFrames options"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPPMuw2PXGeo"
      },
      "outputs": [],
      "source": [
        "bf.options.bigquery.project = PROJECT_ID\n",
        "bf.options.bigquery.location = REGION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDfrKwMKE_dK"
      },
      "source": [
        "If you want to reset the location of the created DataFrame or Series objects, reset the session by executing `bf.reset_session()`. After that, you can reuse `bf.options.bigquery.location` to specify another location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjfRpSruzg5j"
      },
      "source": [
        "## Import data into BigQuery DataFrames\n",
        "\n",
        "You can create a DataFrame by reading data from a BigQuery table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d86W4hNqzZJb"
      },
      "outputs": [],
      "source": [
        "df = bf.read_gbq(\"bigquery-public-data.ml_datasets.penguins\")\n",
        "df = df.dropna()\n",
        "\n",
        "# BigQuery DataFrames creates a default numbered index, which we can give a name\n",
        "df.index.name = \"penguin_id\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDfCJ6-LkRB1"
      },
      "source": [
        "Take a look at a few rows of the DataFrame:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arGaUZVWkSwT"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkUIcMXPkahu"
      },
      "source": [
        "## Clean and prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DScncEoDkiTG"
      },
      "source": [
        "We're are going to start with supervised learning, where a Linear Regression model will learn to predict the body mass (output variable `y`) using input features such as flipper length, sex, species, and more (features `X`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9mW93o9z_-L"
      },
      "outputs": [],
      "source": [
        "# Isolate input features and output variable into DataFrames\n",
        "X = df[['island', 'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'sex', 'species']]\n",
        "y = df[['body_mass_g']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkw0Cs62k_cl"
      },
      "source": [
        "Part of preparing data for a machine learning task is splitting it into subsets for training and testing to ensure that the solution is not overfitting. By default, BQML will automatically manage splitting the data for you. However, BQML also supports manually splitting out your training data.\n",
        "\n",
        "Performing a manual data split can be done with `bigframes.ml.model_selection.train_test_split` like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NysWAWmvlAxB"
      },
      "outputs": [],
      "source": [
        "from bigframes.ml.model_selection import train_test_split\n",
        "\n",
        "# This will split X and y into test and training sets, with 20% of the rows in the test set,\n",
        "# and the rest in the training set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "  X, y, test_size=0.2)\n",
        "\n",
        "# Show the shape of the data after the split\n",
        "print(f\"\"\"X_train shape: {X_train.shape}\n",
        "X_test shape: {X_test.shape}\n",
        "y_train shape: {y_train.shape}\n",
        "y_test shape: {y_test.shape}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faFnVnNolydu"
      },
      "source": [
        "If we look at the data, we can see that random rows were selected for\n",
        "each side of the split:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8bz1HwLlyLP"
      },
      "outputs": [],
      "source": [
        "X_test.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4ic7GQEl67Y"
      },
      "source": [
        "Note that the `y_test` data matches the same rows in `X_test`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PflbhKGkl8v2"
      },
      "outputs": [],
      "source": [
        "y_test.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkf52IdvmSaj"
      },
      "source": [
        "## Estimators\n",
        "\n",
        "Following Scikit-Learn, all learning components are \"estimators\"; objects that can learn from training data and then apply themselves to new data. Estimators share the following patterns:\n",
        "\n",
        "- a constructor that takes a list of parameters\n",
        "- a standard string representation that shows the class name and all non-default parameters, e.g. `LinearRegression(fit_intercept=False)`\n",
        "- a `.fit(..)` method to fit the estimator to training data\n",
        "\n",
        "There estimators can be further broken down into two main subtypes:\n",
        " 1. Transformers\n",
        " 2. Predictors\n",
        "\n",
        "Let's walk through each of these with our example model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55oNSWQ2Q5te"
      },
      "source": [
        "### Transformers\n",
        "\n",
        "Transformers are estimators that are used to prepare data for consumption by other estimators ('preprocessing'). In addition to `.fit(...)`, the transformer implements a `.transform(...)` method, which will apply a transformation based on what was computed during `.fit(..)`. With this pattern dynamic preprocessing steps can be applied to both training and test/production data consistently.\n",
        "\n",
        "An example of a transformer is `bigframes.ml.preprocessing.StandardScaler`, which rescales a dataset to have a mean of zero and a standard deviation of one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhATDMR-mkdF"
      },
      "outputs": [],
      "source": [
        "from bigframes.ml.preprocessing import StandardScaler\n",
        "\n",
        "# StandardScaler will only work on numeric columns\n",
        "numeric_columns = [\"culmen_length_mm\", \"culmen_depth_mm\", \"flipper_length_mm\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train[numeric_columns])\n",
        "\n",
        "# Now, standardscaler should transform the numbers to have mean of zero\n",
        "# and standard deviation of one:\n",
        "scaler.transform(X_train[numeric_columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhywHzH-ml-W"
      },
      "source": [
        "We can then repeat this transformation on the test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfwSLOTXmspI"
      },
      "outputs": [],
      "source": [
        "scaler.transform(X_test[numeric_columns])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9enAdjzPmwmv"
      },
      "source": [
        "#### Composing transformers\n",
        "\n",
        "To process data where different columns need different preprocessors, `bigframes.composition.ColumnTransformer` can be employed.\n",
        "\n",
        "Let's create an aggregate transform that applies `StandardScalar` to the numeric columns and `OneHotEncoder` to the string columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8Wwx3emmz2J"
      },
      "outputs": [],
      "source": [
        "from bigframes.ml.compose import ColumnTransformer\n",
        "from bigframes.ml.preprocessing import OneHotEncoder\n",
        "\n",
        "# Create an aggregate transform that applies StandardScaler to the numeric columns,\n",
        "# and OneHotEncoder to the string columns\n",
        "preproc = ColumnTransformer([\n",
        "    (\"scale\", StandardScaler(), [\"culmen_length_mm\", \"culmen_depth_mm\", \"flipper_length_mm\"]),\n",
        "    (\"encode\", OneHotEncoder(), [\"species\", \"sex\", \"island\"])])\n",
        "\n",
        "# Now we can fit all columns of the training data\n",
        "preproc.fit(X_train)\n",
        "\n",
        "processed_X_train = preproc.transform(X_train)\n",
        "processed_X_test = preproc.transform(X_test)\n",
        "\n",
        "# View the processed training data\n",
        "processed_X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhoO4fctm4Q5"
      },
      "source": [
        "### Predictors\n",
        "\n",
        "Predictors are estimators that learn and make predictions. In addition to `.fit(...)`, the predictor implements a `.predict(...)` method, which will use what was learned during `.fit(...)` to predict some output.\n",
        "\n",
        "Predictors can be further broken down into two categories:\n",
        "* Supervised predictors\n",
        "* Unsupervised predictors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqLItVyjslP8"
      },
      "source": [
        "#### Supervised predictors\n",
        "\n",
        "Supervised learning is when we train a model on input-output pairs, and then ask it to predict the output for new inputs. An example of such a predictor is `bigframes.ml.linear_models.LinearRegression`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeloMmopm8KI"
      },
      "outputs": [],
      "source": [
        "from bigframes.ml.linear_model import LinearRegression\n",
        "\n",
        "linreg = LinearRegression()\n",
        "\n",
        "# Learn from the training data how to predict output y\n",
        "linreg.fit(processed_X_train, y_train)\n",
        "\n",
        "# Predict y for the test data\n",
        "predicted_y_test = linreg.predict(processed_X_test)\n",
        "\n",
        "# View predictions\n",
        "predicted_y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z42qesW_nAIf"
      },
      "source": [
        "#### Unsupervised predictors\n",
        "\n",
        "In unsupervised learning, there are no known outputs in the training data, instead the model learns on input data alone and predicts something else. An example of an unsupervised predictor is `bigframes.ml.cluster.KMeans`, which learns how to fit input data to a target number of clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M13zd02znCIg"
      },
      "outputs": [],
      "source": [
        "from bigframes.ml.cluster import KMeans\n",
        "\n",
        "# Specify KMeans with four clusters\n",
        "kmeans = KMeans(n_clusters=4)\n",
        "\n",
        "# Fit data\n",
        "kmeans.fit(processed_X_train)\n",
        "\n",
        "# View predictions\n",
        "kmeans.predict(processed_X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFwsIbscnEvh"
      },
      "source": [
        "## Pipelines\n",
        "\n",
        "Transfomers and predictors can be chained into a single estimator component using `bigframes.ml.pipeline.Pipeline`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku2OXqgJnEeR"
      },
      "outputs": [],
      "source": [
        "from bigframes.ml.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "  ('preproc', preproc),\n",
        "  ('linreg', linreg)\n",
        "])\n",
        "\n",
        "# Print our pipeline\n",
        "pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCQCY_6wnKz_"
      },
      "source": [
        "The pipeline simplifies the workflow by applying each of its component steps automatically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsF7FYagnMko"
      },
      "outputs": [],
      "source": [
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "predicted_y_test = pipeline.predict(X_test)\n",
        "predicted_y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiLzpsg8nRXn"
      },
      "source": [
        "In the backend, a pipeline will actually be compiled into a single model with an embedded TRANSFORM step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTzAxTv1nUKZ"
      },
      "source": [
        "## Evaluating results\n",
        "\n",
        "Some models include a convenient `.score(X, y)` method for evaulation with a preset accuracy metric:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8nR1ZqznU-B"
      },
      "outputs": [],
      "source": [
        "# In the case of a pipeline, this will be equivalent to calling .score on the contained LinearRegression\n",
        "pipeline.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHM7jls6nY8A"
      },
      "source": [
        "For a more general approach, the library `bigframes.ml.metrics` is provided:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdEN4Ob9nan4"
      },
      "outputs": [],
      "source": [
        "from bigframes.ml.metrics import r2_score\n",
        "\n",
        "r2_score(y_test, predicted_y_test[\"predicted_body_mass_g\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opn4ycPyneVh"
      },
      "source": [
        "## Save to BigQuery\n",
        "\n",
        "Estimators can be saved to BigQuery as BQML models, and loaded again in future.\n",
        "\n",
        "Saving requires `bigquery.tables.create` permission, and loading requires `bigquery.models.getMetadata` permission.\n",
        "These permissions can be at project level or the dataset level.\n",
        "\n",
        "If you have those permissions, please go ahead and uncomment the code in the following cells and run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb0HpkdpnigJ"
      },
      "outputs": [],
      "source": [
        "linreg.to_gbq(f\"{DATASET}.penguins_model\", replace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zNOBlHdnkII"
      },
      "outputs": [],
      "source": [
        "bf.read_gbq_model(f\"{DATASET}.penguins_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfV-du5uTcBB"
      },
      "source": [
        "We can also save the pipeline to BigQuery. BigQuery will save this as a single model, with the pre-processing steps embedded in the TRANSFORM property:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P76_TQ3IR6nB"
      },
      "outputs": [],
      "source": [
        "pipeline.to_gbq(f\"{DATASET}.penguins_pipeline\", replace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKvlKFjAbToJ"
      },
      "outputs": [],
      "source": [
        "bf.read_gbq_model(f\"{DATASET}.penguins_pipeline\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCsmt0IwFkDy"
      },
      "source": [
        "## Summary and next steps\n",
        "\n",
        "You've completed an end-to-end machine learning workflow using the built-in capabilities of BigQuery DataFrames.\n",
        "\n",
        "Learn more about BigQuery DataFrames in the [documentation](https://cloud.google.com/python/docs/reference/bigframes/latest) and find more sample notebooks in the [GitHub repo](https://github.com/googleapis/python-bigquery-dataframes/tree/main/notebooks)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpV-iwP9qw9c"
      },
      "source": [
        "### Cleaning up\n",
        "\n",
        "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
        "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
        "\n",
        "Otherwise, you can uncomment the remaining cells and run them to delete the individual resources you created in this tutorial:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwumLUKmVpuH"
      },
      "outputs": [],
      "source": [
        "# # Delete the BQML models\n",
        "# MODEL_NAME = f\"{PROJECT_ID}:{DATASET}.penguins_model\"\n",
        "# ! bq rm -f --model {MODEL_NAME}\n",
        "# PIPELINE_NAME = f\"{PROJECT_ID}:{DATASET}.penguins_pipeline\"\n",
        "# ! bq rm -f --model {PIPELINE_NAME}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
