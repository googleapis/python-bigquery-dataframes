{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d77cb8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/google/home/shuowei/src/python-bigquery-dataframes/bigframes/core/global_session.py:103: DefaultLocationWarning: No explicit location is set, so using location US for the session.\n",
      "  _global_session = bigframes.session.connect(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Query job 536d38d9-59d8-49ac-9247-f8d66dccabdc is DONE. 0 Bytes processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=bigframes-dev&j=bq:US:536d38d9-59d8-49ac-9247-f8d66dccabdc&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job d6f4bc43-015f-427c-97a4-4005fcd5dc37 is DONE. 0 Bytes processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=bigframes-dev&j=bq:US:d6f4bc43-015f-427c-97a4-4005fcd5dc37&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 3dc5742c-35a4-45ec-9b1d-a08fd072b7b8 is DONE. 0 Bytes processed. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=bigframes-dev&j=bq:US:3dc5742c-35a4-45ec-9b1d-a08fd072b7b8&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Query job 0b18e95b-0728-448e-b27a-8094d27d8135 is RUNNING. <a target=\"_blank\" href=\"https://console.cloud.google.com/bigquery?project=bigframes-dev&j=bq:US:0b18e95b-0728-448e-b27a-8094d27d8135&page=queryresults\">Open Job</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "Forbidden",
     "evalue": "403 GET https://bigquery.googleapis.com/bigquery/v2/projects/bigframes-dev/queries/0b18e95b-0728-448e-b27a-8094d27d8135?maxResults=0&location=US&prettyPrint=false: Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern. bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist). Please make sure gs://your-bucket/audio-files/* is accessible via appropriate IAM roles, e.g. Storage Object Viewer or Storage Object Creator.\n\nLocation: US\nJob ID: 0b18e95b-0728-448e-b27a-8094d27d8135\n Share your usecase with the BigQuery DataFrames team at the https://bit.ly/bigframes-feedback survey. You are currently running BigFrames version 2.10.0. [{'@type': 'type.googleapis.com/google.rpc.DebugInfo', 'detail': '[ACCESS_DENIED] message=ACCESS_DENIED: [BigQuery, BigQuery, Permission denied while globbing file pattern. bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\'storage.objects.list\\' denied on resource (or it may not exist). Please make sure gs://your-bucket/audio-files/* is accessible via appropriate IAM roles, e.g. Storage Object Viewer or Storage Object Creator.] debug=code: \\t AUTHORIZATION_ERROR\\ndescription: \"Permission denied while globbing file pattern. bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\'storage.objects.list\\\\\\' denied on resource (or it may not exist). Please make sure gs://your-bucket/audio-files/* is accessible via appropriate IAM roles, e.g. Storage Object Viewer or Storage Object Creator. message: \\\\\"GCS aka Bigstore is not approved for storing Google user data (go/blobstore-gcs-getting-started). If you have obtained security exceptions for Bigstore instead of Blobstore, Please make sure Dremel has access to the files and all directories on the path to the files (http://go/dremel-access).\\\\\"\\\\npermission_denied_error {\\\\n  accessed_resource_uri: \\\\\"/bigstore/your-bucket/audio-files/*\\\\\"\\\\n  system: BIGSTORE\\\\n}\\\\nunderlying_status {\\\\n  code: 7\\\\n  space: \\\\\"generic\\\\\"\\\\n  message: \\\\\"Calling Match with file \\\\\\\\\\\\\"/bigstore/your-bucket/audio-files/**\\\\\\\\\\\\\": cloud.bigstore.ResponseCode.ErrorCode::ACCESS_DENIED: bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist). [google.rpc.error_details_ext] { message: \\\\\\\\\\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\\\\\\\\\" details { [type.googleapis.com/google.rpc.DebugInfo] { stack_entries: \\\\\\\\\\\\\"com.google.net.rpc3.client.RpcClientException: APPLICATION_ERROR;cloud.bigstore/FrontendObjectsService.List;bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist).;AppErrorCode=1;StartTimeMs=1752257982548;tcp;Deadline(sec)=29.991333608;ResFormat=uncompressed;interceptors={[com.google.prod.fireaxe.filters.FireaxeRpcClientInterceptorImpl;com.google.cloud.bigstore.common.LatencyCollectingInterceptor;com.google.frameworks.debug.sherlog.core.rpcutil.Stubby3ClientInterceptor];overrides={}};ServerTimeSec=0.021769909;LogBytes=256;Non-FailFast;EffSecLevel=strong_privacy_and_integrity;ReqFormat=uncompressed;ReqID=f0298bdab819b7ef;GlobalID=632dd03b377a3319;Server=[::1]:14003\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.net.rpc3.client.RpcStub.startBlockingRpcInternal(RpcStub.java:571)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.net.rpc3.client.RpcStub.startBlockingRpc(RpcStub.java:471)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.proto.BackendObjectsService$Stub.list(BackendObjectsService.java:734)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.common.LampreyServiceBase.call(LampreyServiceBase.java:37)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.common.BigstoreFrontendObjectsServiceImpl.call(BigstoreFrontendObjectsServiceImpl.java:40)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.common.BigstoreFrontendObjectsServiceImpl.list(BigstoreFrontendObjectsServiceImpl.java:97)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.stubby.BigstoreStubbyImpl.lambda$listObjects$0(BigstoreStubbyImpl.java:832)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.common.CallbackRequestHandler.handleCallbackRequest(CallbackRequestHandler.java:288)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.common.CallbackRequestHandler.handleCallbackRequest(CallbackRequestHandler.java:158)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.stubby.BigstoreStubbyImpl.listObjects(BigstoreStubbyImpl.java:821)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.common.ProxyDelegatorBase.callAndRecordLatency(ProxyDelegatorBase.java:109)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.stubby.StubbyProxyDelegator.call(StubbyProxyDelegator.java:184)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.stubby.BigstoreStubbyProxy.listObjects(BigstoreStubbyProxy.java:334)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.isolation.RpcReceiver.lambda$processRequestAsync$0(RpcReceiver.java:198)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.isolation.AsyncExecutor.lambda$submit$0(AsyncExecutor.java:213)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.common.context.ContextRunnable.runInContext(ContextRunnable.java:83)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"io.grpc.Context.run(Context.java:536)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.tracing.GenericContextCallback.runInInheritedContext(GenericContextCallback.java:58)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.common.context.ContextRunnable.run(ContextRunnable.java:74)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"java.base/java.lang.Thread.run(Unknown Source)\\\\\\\\\\\\\" } } } [blobstore2.GcsErrorDetails] { xml_code: \\\\\\\\\\\\\"AccessDenied\\\\\\\\\\\\\" msg: \\\\\\\\\\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\\\\\\\\\" http_code: 403 details { first: \\\\\\\\\\\\\"Details\\\\\\\\\\\\\" second: \\\\\\\\\\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\\\\\\\\\" } debug_info: \\\\\\\\\\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\\\\\\\\\" } [cloud.bigstore.GcsLatencyInfo] { requests { method: \\\\\\\\\\\\\"/BackendObjectsService.List\\\\\\\\\\\\\" deadline { seconds: 29 nanos: 979569783 } start { seconds: 1752257982 nanos: 549935579 } end { seconds: 1752257982 nanos: 570470964 } status { code: 1 space: \\\\\\\\\\\\\"cloud.bigstore.ResponseCode.ErrorCode\\\\\\\\\\\\\" message: \\\\\\\\\\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\\\\\\\\\" canonical_code: 7 } } requests { method: \\\\\\\\\\\\\"/BucketMdService.LookupBucket\\\\\\\\\\\\\" deadline { seconds: 4 nanos: 999967650 } start { seconds: 1752257982 nanos: 551314439 } end { seconds: 1752257982 nanos: 553833108 } status { } metadata_spanner_stats { read_walltime_millis: 0 read_cpu_millis: 0 read_scheduler_delay_millis: 0 read_throttle_delay_millis: 0 read_per_service_limit_queue_delay_millis: 0 read_locking_delay_millis: 0 read_client_overhead_delay_millis: 0 read_client_flow_control_delay_millis: 0 read_io_delay_millis: 0 } elapsed_time_isolator_metrics { } sunspot_verdict: VERDICT_CAT_UNKNOWN } requests { method: \\\\\\\\\\\\\"/AccessService.CheckListObjects\\\\\\\\\\\\\" deadline { seconds: 29 nanos: 969642965 } start { seconds: 1752257982 nanos: 553970188 } end { seconds: 1752257982 nanos: 569602062 } status { } } }\\\\\"\\\\n  message_set {\\\\n    [google.rpc.error_details_ext] {\\\\n      message: \\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\"\\\\n    }\\\\n  }\\\\n}\\\\nerror_context {\\\\n  table_name: \\\\\"bigframes-dev._8b037bfb7316dddf9d92b12dcf93e008906bfe52.bqdf20250711_sessionee050e_bb3969eb60f548be8a4b6f5e2564f69f\\\\\"\\\\n}\\\\n\"\\ncause: USER_ERROR\\naddress: \"http://jfs5.prod.google.com:4901/task?handle=logs.2071.serving.shard-mals.cloud-dataengine.11785653463719 Partition description: __SHUFFLE0/0 TableDef \\\\\\'bigframes-dev._8b037bfb7316dddf9d92b12dcf93e008906bfe52.bqdf20250711_sessionee050e_bb3969eb60f548be8a4b6f5e2564f69f\\\\\\' of type \\\\\\'object-meta\\\\\\': /bigstore/your-bucket/audio-files/*\"\\nstatus_proto {\\n  code: 7\\n  space: \"generic\"\\n  message: \"Permission denied while globbing file pattern. bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\'storage.objects.list\\\\\\' denied on resource (or it may not exist). Please make sure gs://your-bucket/audio-files/* is accessible via appropriate IAM roles, e.g. Storage Object Viewer or Storage Object Creator.\"\\n}\\nerror_details {\\n  message: \"GCS aka Bigstore is not approved for storing Google user data (go/blobstore-gcs-getting-started). If you have obtained security exceptions for Bigstore instead of Blobstore, Please make sure Dremel has access to the files and all directories on the path to the files (http://go/dremel-access).\"\\n  permission_denied_error {\\n    accessed_resource_uri: \"/bigstore/your-bucket/audio-files/*\"\\n    system: BIGSTORE\\n  }\\n  underlying_status {\\n    code: 7\\n    space: \"generic\"\\n    message: \"Calling Match with file \\\\\"/bigstore/your-bucket/audio-files/**\\\\\": cloud.bigstore.ResponseCode.ErrorCode::ACCESS_DENIED: bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\'storage.objects.list\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\'storage.objects.list\\\\\\' denied on resource (or it may not exist). [google.rpc.error_details_ext] { message: \\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\" details { [type.googleapis.com/google.rpc.DebugInfo] { stack_entries: \\\\\"com.google.net.rpc3.client.RpcClientException: APPLICATION_ERROR;cloud.bigstore/FrontendObjectsService.List;bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).;AppErrorCode=1;StartTimeMs=1752257982548;tcp;Deadline(sec)=29.991333608;ResFormat=uncompressed;interceptors={[com.google.prod.fireaxe.filters.FireaxeRpcClientInterceptorImpl;com.google.cloud.bigstore.common.LatencyCollectingInterceptor;com.google.frameworks.debug.sherlog.core.rpcutil.Stubby3ClientInterceptor];overrides={}};ServerTimeSec=0.021769909;LogBytes=256;Non-FailFast;EffSecLevel=strong_privacy_and_integrity;ReqFormat=uncompressed;ReqID=f0298bdab819b7ef;GlobalID=632dd03b377a3319;Server=[::1]:14003\\\\\" stack_entries: \\\\\"com.google.net.rpc3.client.RpcStub.startBlockingRpcInternal(RpcStub.java:571)\\\\\" stack_entries: \\\\\"com.google.net.rpc3.client.RpcStub.startBlockingRpc(RpcStub.java:471)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.proto.BackendObjectsService$Stub.list(BackendObjectsService.java:734)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.common.LampreyServiceBase.call(LampreyServiceBase.java:37)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.common.BigstoreFrontendObjectsServiceImpl.call(BigstoreFrontendObjectsServiceImpl.java:40)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.common.BigstoreFrontendObjectsServiceImpl.list(BigstoreFrontendObjectsServiceImpl.java:97)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.stubby.BigstoreStubbyImpl.lambda$listObjects$0(BigstoreStubbyImpl.java:832)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.common.CallbackRequestHandler.handleCallbackRequest(CallbackRequestHandler.java:288)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.common.CallbackRequestHandler.handleCallbackRequest(CallbackRequestHandler.java:158)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.stubby.BigstoreStubbyImpl.listObjects(BigstoreStubbyImpl.java:821)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.common.ProxyDelegatorBase.callAndRecordLatency(ProxyDelegatorBase.java:109)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.stubby.StubbyProxyDelegator.call(StubbyProxyDelegator.java:184)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.stubby.BigstoreStubbyProxy.listObjects(BigstoreStubbyProxy.java:334)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.isolation.RpcReceiver.lambda$processRequestAsync$0(RpcReceiver.java:198)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.isolation.AsyncExecutor.lambda$submit$0(AsyncExecutor.java:213)\\\\\" stack_entries: \\\\\"com.google.common.context.ContextRunnable.runInContext(ContextRunnable.java:83)\\\\\" stack_entries: \\\\\"io.grpc.Context.run(Context.java:536)\\\\\" stack_entries: \\\\\"com.google.tracing.GenericContextCallback.runInInheritedContext(GenericContextCallback.java:58)\\\\\" stack_entries: \\\\\"com.google.common.context.ContextRunnable.run(ContextRunnable.java:74)\\\\\" stack_entries: \\\\\"java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\\\\" stack_entries: \\\\\"java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\\\\" stack_entries: \\\\\"java.base/java.lang.Thread.run(Unknown Source)\\\\\" } } } [blobstore2.GcsErrorDetails] { xml_code: \\\\\"AccessDenied\\\\\" msg: \\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\" http_code: 403 details { first: \\\\\"Details\\\\\" second: \\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\" } debug_info: \\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\" } [cloud.bigstore.GcsLatencyInfo] { requests { method: \\\\\"/BackendObjectsService.List\\\\\" deadline { seconds: 29 nanos: 979569783 } start { seconds: 1752257982 nanos: 549935579 } end { seconds: 1752257982 nanos: 570470964 } status { code: 1 space: \\\\\"cloud.bigstore.ResponseCode.ErrorCode\\\\\" message: \\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\" canonical_code: 7 } } requests { method: \\\\\"/BucketMdService.LookupBucket\\\\\" deadline { seconds: 4 nanos: 999967650 } start { seconds: 1752257982 nanos: 551314439 } end { seconds: 1752257982 nanos: 553833108 } status { } metadata_spanner_stats { read_walltime_millis: 0 read_cpu_millis: 0 read_scheduler_delay_millis: 0 read_throttle_delay_millis: 0 read_per_service_limit_queue_delay_millis: 0 read_locking_delay_millis: 0 read_client_overhead_delay_millis: 0 read_client_flow_control_delay_millis: 0 read_io_delay_millis: 0 } elapsed_time_isolator_metrics { } sunspot_verdict: VERDICT_CAT_UNKNOWN } requests { method: \\\\\"/AccessService.CheckListObjects\\\\\" deadline { seconds: 29 nanos: 969642965 } start { seconds: 1752257982 nanos: 553970188 } end { seconds: 1752257982 nanos: 569602062 } status { } } }\"\\n    message_set {\\n      [google.rpc.error_details_ext] {\\n        message: \"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\'storage.objects.list\\\\\\' denied on resource (or it may not exist).\"\\n      }\\n    }\\n  }\\n  error_context {\\n    table_name: \"bigframes-dev._8b037bfb7316dddf9d92b12dcf93e008906bfe52.bqdf20250711_sessionee050e_bb3969eb60f548be8a4b6f5e2564f69f\"\\n  }\\n}\\n errorProto=code: \"ACCESS_DENIED\"\\nargument: \"BigQuery\"\\nargument: \"BigQuery\"\\nargument: \"Permission denied while globbing file pattern. bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\'storage.objects.list\\\\\\' denied on resource (or it may not exist). Please make sure gs://your-bucket/audio-files/* is accessible via appropriate IAM roles, e.g. Storage Object Viewer or Storage Object Creator.\"\\nlocation_type: OTHER\\nlocation: \"gs://your-bucket/audio-files/*\"\\n\\n\\tat com.google.cloud.helix.common.Exceptions.fromProto(Exceptions.java:2016)\\n\\tat com.google.cloud.helix.common.dremel.QueryExecutorImpl.mapDremelErrorsTohelixException(QueryExecutorImpl.java:1194)\\n\\tat com.google.cloud.helix.common.dremel.QueryExecutorImpl$ConfiguredQueryMigration$StreamHandler.onMessage(QueryExecutorImpl.java:769)\\n\\tat com.google.cloud.helix.common.dremel.QueryExecutorImpl$ConfiguredQueryMigration$StreamHandler.onMessage(QueryExecutorImpl.java:695)\\n\\tat com.google.net.rpc3.stream.RpcMessageCallback$ForwardingRpcMessageCallback.onMessage(RpcMessageCallback.java:128)\\n\\tat com.google.net.rpc3.impl.RpcStreamInternalContext.processMessageUnlocked(RpcStreamInternalContext.java:1852)\\n\\tat com.google.net.rpc3.impl.RpcStreamInternalContext.invokeCallbacksInternalUnlocked(RpcStreamInternalContext.java:2904)\\n\\tat com.google.net.rpc3.impl.RpcStreamInternalContext.invokeCallbacksUnlocked(RpcStreamInternalContext.java:2830)\\n\\tat com.google.net.eventmanager.AbstractFutureTask$Sync.innerRun(AbstractFutureTask.java:259)\\n\\tat com.google.net.eventmanager.AbstractFutureTask.run(AbstractFutureTask.java:120)\\n\\tat com.google.net.eventmanager.EventManagerImpl.runTaskTraced(EventManagerImpl.java:900)\\n\\tat com.google.net.eventmanager.EventManagerImpl.runTask(EventManagerImpl.java:892)\\n\\tat com.google.net.eventmanager.EventManagerImpl.internalRunWorkerLoop(EventManagerImpl.java:1319)\\n\\tat com.google.net.eventmanager.EventManagerImpl.runWorkerLoop(EventManagerImpl.java:1210)\\n\\tat com.google.net.eventmanager.WorkerThreadInfo.runWorkerLoop(WorkerThreadInfo.java:153)\\n\\tat com.google.net.eventmanager.EventManagerImpl$WorkerThread.run(EventManagerImpl.java:1999)\\n'}]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m      9\u001b[0m flattened \u001b[38;5;241m=\u001b[39m bpd\u001b[38;5;241m.\u001b[39mfrom_glob_path(  \n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://your-bucket/audio-files/*\u001b[39m\u001b[38;5;124m\"\u001b[39m,   \n\u001b[1;32m     11\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGCS Blob\u001b[39m\u001b[38;5;124m\"\u001b[39m  \n\u001b[1;32m     12\u001b[0m )  \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Alternatively, create from URI strings with null index  \u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# df = bpd.DataFrame({\"uri\": [\"gs://bucket/audio1.wav\", \"gs://bucket/audio2.wav\"]})  \u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# df[\"GCS Blob\"] = df[\"uri\"].str.to_blob()  \u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# flattened = bpd.read_gbq_table(\"your_table\", index_col=bigframes.enums.DefaultIndexKind.NULL)  \u001b[39;00m\n\u001b[1;32m     18\u001b[0m   \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 3. This will trigger the NullIndexError  \u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m flattened[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTranscription\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mflattened\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGCS Blob\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maudio_transcribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgemini-2.0-flash-001\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/core/log_adapter.py:175\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m _call_stack\u001b[38;5;241m.\u001b[39mappend(full_method_name)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mNotImplementedError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# Log method parameters that are implemented in pandas but either missing (TypeError)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# or not fully supported (NotImplementedError) in BigFrames.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# Logging is currently supported only when we can access the bqclient through\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# _block.session.bqclient.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_call_stack) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/operations/blob.py:822\u001b[0m, in \u001b[0;36mBlobAccessor.audio_transcribe\u001b[0;34m(self, engine, connection, model_name, verbose)\u001b[0m\n\u001b[1;32m    815\u001b[0m llm_model \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39mGeminiTextGenerator(\n\u001b[1;32m    816\u001b[0m     model_name\u001b[38;5;241m=\u001b[39mmodel_name,\n\u001b[1;32m    817\u001b[0m     session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_block\u001b[38;5;241m.\u001b[39msession,\n\u001b[1;32m    818\u001b[0m     connection_name\u001b[38;5;241m=\u001b[39mconnection,\n\u001b[1;32m    819\u001b[0m )\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# transcribe audio using ML.GENERATE_TEXT\u001b[39;00m\n\u001b[0;32m--> 822\u001b[0m transcribed_results \u001b[38;5;241m=\u001b[39m \u001b[43mllm_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudio_series\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprompt_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio_series\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m transcribed_content_series \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    829\u001b[0m     bpd\u001b[38;5;241m.\u001b[39mSeries, transcribed_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mml_generate_text_llm_result\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    830\u001b[0m )\u001b[38;5;241m.\u001b[39mrename(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscribed_content\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/core/log_adapter.py:175\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m _call_stack\u001b[38;5;241m.\u001b[39mappend(full_method_name)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mNotImplementedError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# Log method parameters that are implemented in pandas but either missing (TypeError)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# or not fully supported (NotImplementedError) in BigFrames.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# Logging is currently supported only when we can access the bqclient through\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# _block.session.bqclient.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_call_stack) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/ml/llm.py:745\u001b[0m, in \u001b[0;36mGeminiTextGenerator.predict\u001b[0;34m(self, X, temperature, max_output_tokens, top_k, top_p, ground_with_google_search, max_retries, prompt, output_schema)\u001b[0m\n\u001b[1;32m    737\u001b[0m     options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_schema\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m output_schema\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_and_retry(\n\u001b[1;32m    739\u001b[0m         core\u001b[38;5;241m.\u001b[39mBqmlModel\u001b[38;5;241m.\u001b[39mgenerate_table_tvf,\n\u001b[1;32m    740\u001b[0m         X,\n\u001b[1;32m    741\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    742\u001b[0m         max_retries\u001b[38;5;241m=\u001b[39mmax_retries,\n\u001b[1;32m    743\u001b[0m     )\n\u001b[0;32m--> 745\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_and_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBqmlModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_text_tvf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    750\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/ml/base.py:266\u001b[0m, in \u001b[0;36mRetriableRemotePredictor._predict_and_retry\u001b[0;34m(self, bqml_model_predict_tvf, X, options, max_retries)\u001b[0m\n\u001b[1;32m    263\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mbqml_model_predict_tvf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtvf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bqml_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_fail\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m success \u001b[38;5;241m=\u001b[39m df[bqml_model_predict_tvf\u001b[38;5;241m.\u001b[39mstatus_col]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlen() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    269\u001b[0m df_succ \u001b[38;5;241m=\u001b[39m df[success]\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/ml/core.py:179\u001b[0m, in \u001b[0;36mBqmlModel.generate_text\u001b[0;34m(self, input_data, options)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_text\u001b[39m(\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    175\u001b[0m     input_data: bpd\u001b[38;5;241m.\u001b[39mDataFrame,\n\u001b[1;32m    176\u001b[0m     options: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m]],\n\u001b[1;32m    177\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m bpd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m    178\u001b[0m     options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflatten_json_output\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_ml_tvf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msource_sql\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sql_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mml_generate_text\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m            \u001b[49m\u001b[43msource_sql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_sql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstruct_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/ml/core.py:98\u001b[0m, in \u001b[0;36mBqmlModel._apply_ml_tvf\u001b[0;34m(self, input_data, apply_sql_tvf)\u001b[0m\n\u001b[1;32m     93\u001b[0m input_sql, index_col_ids, index_labels \u001b[38;5;241m=\u001b[39m input_data\u001b[38;5;241m.\u001b[39m_to_sql_query(\n\u001b[1;32m     94\u001b[0m     include_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     95\u001b[0m )\n\u001b[1;32m     97\u001b[0m result_sql \u001b[38;5;241m=\u001b[39m apply_sql_tvf(input_sql)\n\u001b[0;32m---> 98\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_gbq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_sql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39m_has_index:\n\u001b[1;32m    100\u001b[0m     df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mnames \u001b[38;5;241m=\u001b[39m index_labels\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/core/log_adapter.py:175\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m _call_stack\u001b[38;5;241m.\u001b[39mappend(full_method_name)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mNotImplementedError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# Log method parameters that are implemented in pandas but either missing (TypeError)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# or not fully supported (NotImplementedError) in BigFrames.\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# Logging is currently supported only when we can access the bqclient through\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;66;03m# _block.session.bqclient.\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_call_stack) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/session/__init__.py:439\u001b[0m, in \u001b[0;36mSession.read_gbq\u001b[0;34m(self, query_or_table, index_col, columns, configuration, max_results, filters, use_cache, col_order, dry_run)\u001b[0m\n\u001b[1;32m    436\u001b[0m     columns \u001b[38;5;241m=\u001b[39m col_order\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bf_io_bigquery\u001b[38;5;241m.\u001b[39mis_query(query_or_table):\n\u001b[0;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_gbq_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore # for dry_run overload\u001b[39;49;00m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_or_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdry_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdry_run\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m configuration \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/session/loader.py:996\u001b[0m, in \u001b[0;36mGbqDataLoader.read_gbq_query\u001b[0;34m(self, query, index_col, columns, configuration, max_results, use_cache, filters, dry_run, force_total_order, allow_large_results)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;66;03m# TODO(b/421161077): If an explicit destination table is set in\u001b[39;00m\n\u001b[1;32m    994\u001b[0m \u001b[38;5;66;03m# configuration, should we respect that setting?\u001b[39;00m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m allow_large_results:\n\u001b[0;32m--> 996\u001b[0m     destination, query_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_query_to_destination\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# No cluster candidates as user query might not be clusterable\u001b[39;49;00m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# (eg because of ORDER BY clause)\u001b[39;49;00m\n\u001b[1;32m   1000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcluster_candidates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfiguration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1003\u001b[0m     query_job_for_metrics \u001b[38;5;241m=\u001b[39m query_job\n\u001b[1;32m   1004\u001b[0m     rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/session/loader.py:1120\u001b[0m, in \u001b[0;36mGbqDataLoader._query_to_destination\u001b[0;34m(self, query, cluster_candidates, configuration, do_clustering)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;66;03m# Write to temp table to workaround BigQuery 10 GB query results\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;66;03m# limit. See: internal issue 303057336.\u001b[39;00m\n\u001b[1;32m   1119\u001b[0m     job_config\u001b[38;5;241m.\u001b[39mlabels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror_caught\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1120\u001b[0m     query_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_start_query_with_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query_job\u001b[38;5;241m.\u001b[39mdestination, query_job\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m google\u001b[38;5;241m.\u001b[39mapi_core\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mBadRequest:\n\u001b[1;32m   1127\u001b[0m     \u001b[38;5;66;03m# Some SELECT statements still aren't compatible with cluster\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m     \u001b[38;5;66;03m# tables as the destination. For example, if the query has a\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# top-level ORDER BY, this conflicts with our ability to cluster\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# the table by the index column(s).\u001b[39;00m\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/session/loader.py:1186\u001b[0m, in \u001b[0;36mGbqDataLoader._start_query_with_job\u001b[0;34m(self, sql, job_config, timeout)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;124;03mStarts BigQuery query job and waits for results.\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \n\u001b[1;32m   1183\u001b[0m \u001b[38;5;124;03mDo not execute dataframe through this API, instead use the executor.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m job_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_job_config(job_config)\n\u001b[0;32m-> 1186\u001b[0m _, query_job \u001b[38;5;241m=\u001b[39m \u001b[43mbf_io_bigquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_query_with_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bqclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1188\u001b[0m \u001b[43m    \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_with_job\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_job\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/session/_io/bigquery/__init__.py:314\u001b[0m, in \u001b[0;36mstart_query_with_client\u001b[0;34m(bq_client, sql, job_config, location, project, timeout, metrics, query_with_job)\u001b[0m\n\u001b[1;32m    312\u001b[0m opts \u001b[38;5;241m=\u001b[39m bigframes\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mdisplay\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opts\u001b[38;5;241m.\u001b[39mprogress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m query_job\u001b[38;5;241m.\u001b[39mconfiguration\u001b[38;5;241m.\u001b[39mdry_run:\n\u001b[0;32m--> 314\u001b[0m     results_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mformatting_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_for_query_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_job\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    319\u001b[0m     results_iterator \u001b[38;5;241m=\u001b[39m query_job\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/bigframes/formatting_helpers.py:149\u001b[0m, in \u001b[0;36mwait_for_query_job\u001b[0;34m(query_job, max_results, page_size, progress_bar)\u001b[0m\n\u001b[1;32m    147\u001b[0m loading_bar \u001b[38;5;241m=\u001b[39m display\u001b[38;5;241m.\u001b[39mHTML(get_query_job_loading_html(query_job))\n\u001b[1;32m    148\u001b[0m display\u001b[38;5;241m.\u001b[39mdisplay(loading_bar, display_id\u001b[38;5;241m=\u001b[39mdisplay_id)\n\u001b[0;32m--> 149\u001b[0m query_result \u001b[38;5;241m=\u001b[39m \u001b[43mquery_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_size\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m query_job\u001b[38;5;241m.\u001b[39mreload()\n\u001b[1;32m    153\u001b[0m display\u001b[38;5;241m.\u001b[39mupdate_display(\n\u001b[1;32m    154\u001b[0m     display\u001b[38;5;241m.\u001b[39mHTML(get_query_job_loading_html(query_job)),\n\u001b[1;32m    155\u001b[0m     display_id\u001b[38;5;241m=\u001b[39mdisplay_id,\n\u001b[1;32m    156\u001b[0m )\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/cloud/bigquery/job/query.py:1696\u001b[0m, in \u001b[0;36mQueryJob.result\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[1;32m   1691\u001b[0m     remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;66;03m# long-running API, don't delay the next request at all.\u001b[39;00m\n\u001b[0;32m-> 1696\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_job_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1697\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001b[39;00m\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    293\u001b[0m )\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(next_sleep)\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py:214\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    209\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    210\u001b[0m         error_list,\n\u001b[1;32m    211\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    212\u001b[0m         original_timeout,\n\u001b[1;32m    213\u001b[0m     )\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/cloud/bigquery/job/query.py:1665\u001b[0m, in \u001b[0;36mQueryJob.result.<locals>.is_job_done\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1661\u001b[0m \u001b[38;5;66;03m# Call jobs.getQueryResults with max results set to 0 just to\u001b[39;00m\n\u001b[1;32m   1662\u001b[0m \u001b[38;5;66;03m# wait for the query to finish. Unlike most methods,\u001b[39;00m\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;66;03m# jobs.getQueryResults hangs as long as it can to ensure we\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;66;03m# know when the query has finished as soon as possible.\u001b[39;00m\n\u001b[0;32m-> 1665\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reload_query_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mreload_query_results_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[38;5;66;03m# Even if the query is finished now according to\u001b[39;00m\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;66;03m# jobs.getQueryResults, we'll want to reload the job status if\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;66;03m# it's not already DONE.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/cloud/bigquery/job/query.py:1463\u001b[0m, in \u001b[0;36mQueryJob._reload_query_results\u001b[0;34m(self, retry, timeout, page_size)\u001b[0m\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transport_timeout, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m)):\n\u001b[1;32m   1461\u001b[0m         transport_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1463\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_query_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_query_results\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransport_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/cloud/bigquery/client.py:2060\u001b[0m, in \u001b[0;36mClient._get_query_results\u001b[0;34m(self, job_id, retry, project, timeout_ms, location, timeout, page_size)\u001b[0m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;66;03m# This call is typically made in a polling loop that checks whether the\u001b[39;00m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;66;03m# job is complete (from QueryJob.done(), called ultimately from\u001b[39;00m\n\u001b[1;32m   2058\u001b[0m \u001b[38;5;66;03m# QueryJob.result()). So we don't need to poll here.\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m span_attributes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m: path}\n\u001b[0;32m-> 2060\u001b[0m resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2062\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspan_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBigQuery.getQueryResults\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspan_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspan_attributes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2068\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2069\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _QueryResults\u001b[38;5;241m.\u001b[39mfrom_api_repr(resource)\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/cloud/bigquery/client.py:858\u001b[0m, in \u001b[0;36mClient._call_api\u001b[0;34m(self, retry, span_name, span_attributes, job_ref, headers, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m create_span(\n\u001b[1;32m    856\u001b[0m         name\u001b[38;5;241m=\u001b[39mspan_name, attributes\u001b[38;5;241m=\u001b[39mspan_attributes, client\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, job_ref\u001b[38;5;241m=\u001b[39mjob_ref\n\u001b[1;32m    857\u001b[0m     ):\n\u001b[0;32m--> 858\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call()\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:294\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    291\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    293\u001b[0m )\n\u001b[0;32m--> 294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:156\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     next_sleep \u001b[38;5;241m=\u001b[39m \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(next_sleep)\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/api_core/retry/retry_base.py:214\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    209\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    210\u001b[0m         error_list,\n\u001b[1;32m    211\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    212\u001b[0m         original_timeout,\n\u001b[1;32m    213\u001b[0m     )\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    216\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/api_core/retry/retry_unary.py:147\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    149\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m~/src/python-bigquery-dataframes/venv/lib/python3.10/site-packages/google/cloud/_http/__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[0;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[1;32m    482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[1;32m    491\u001b[0m )\n\u001b[1;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[0;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expect_json \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[0;31mForbidden\u001b[0m: 403 GET https://bigquery.googleapis.com/bigquery/v2/projects/bigframes-dev/queries/0b18e95b-0728-448e-b27a-8094d27d8135?maxResults=0&location=US&prettyPrint=false: Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern. bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist). Please make sure gs://your-bucket/audio-files/* is accessible via appropriate IAM roles, e.g. Storage Object Viewer or Storage Object Creator.\n\nLocation: US\nJob ID: 0b18e95b-0728-448e-b27a-8094d27d8135\n Share your usecase with the BigQuery DataFrames team at the https://bit.ly/bigframes-feedback survey. You are currently running BigFrames version 2.10.0. [{'@type': 'type.googleapis.com/google.rpc.DebugInfo', 'detail': '[ACCESS_DENIED] message=ACCESS_DENIED: [BigQuery, BigQuery, Permission denied while globbing file pattern. bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\'storage.objects.list\\' denied on resource (or it may not exist). Please make sure gs://your-bucket/audio-files/* is accessible via appropriate IAM roles, e.g. Storage Object Viewer or Storage Object Creator.] debug=code: \\t AUTHORIZATION_ERROR\\ndescription: \"Permission denied while globbing file pattern. bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\'storage.objects.list\\\\\\' denied on resource (or it may not exist). Please make sure gs://your-bucket/audio-files/* is accessible via appropriate IAM roles, e.g. Storage Object Viewer or Storage Object Creator. message: \\\\\"GCS aka Bigstore is not approved for storing Google user data (go/blobstore-gcs-getting-started). If you have obtained security exceptions for Bigstore instead of Blobstore, Please make sure Dremel has access to the files and all directories on the path to the files (http://go/dremel-access).\\\\\"\\\\npermission_denied_error {\\\\n  accessed_resource_uri: \\\\\"/bigstore/your-bucket/audio-files/*\\\\\"\\\\n  system: BIGSTORE\\\\n}\\\\nunderlying_status {\\\\n  code: 7\\\\n  space: \\\\\"generic\\\\\"\\\\n  message: \\\\\"Calling Match with file \\\\\\\\\\\\\"/bigstore/your-bucket/audio-files/**\\\\\\\\\\\\\": cloud.bigstore.ResponseCode.ErrorCode::ACCESS_DENIED: bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist). [google.rpc.error_details_ext] { message: \\\\\\\\\\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\\\\\\\\\" details { [type.googleapis.com/google.rpc.DebugInfo] { stack_entries: \\\\\\\\\\\\\"com.google.net.rpc3.client.RpcClientException: APPLICATION_ERROR;cloud.bigstore/FrontendObjectsService.List;bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist).;AppErrorCode=1;StartTimeMs=1752257982548;tcp;Deadline(sec)=29.991333608;ResFormat=uncompressed;interceptors={[com.google.prod.fireaxe.filters.FireaxeRpcClientInterceptorImpl;com.google.cloud.bigstore.common.LatencyCollectingInterceptor;com.google.frameworks.debug.sherlog.core.rpcutil.Stubby3ClientInterceptor];overrides={}};ServerTimeSec=0.021769909;LogBytes=256;Non-FailFast;EffSecLevel=strong_privacy_and_integrity;ReqFormat=uncompressed;ReqID=f0298bdab819b7ef;GlobalID=632dd03b377a3319;Server=[::1]:14003\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.net.rpc3.client.RpcStub.startBlockingRpcInternal(RpcStub.java:571)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.net.rpc3.client.RpcStub.startBlockingRpc(RpcStub.java:471)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.proto.BackendObjectsService$Stub.list(BackendObjectsService.java:734)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.common.LampreyServiceBase.call(LampreyServiceBase.java:37)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.common.BigstoreFrontendObjectsServiceImpl.call(BigstoreFrontendObjectsServiceImpl.java:40)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.common.BigstoreFrontendObjectsServiceImpl.list(BigstoreFrontendObjectsServiceImpl.java:97)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.stubby.BigstoreStubbyImpl.lambda$listObjects$0(BigstoreStubbyImpl.java:832)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.common.CallbackRequestHandler.handleCallbackRequest(CallbackRequestHandler.java:288)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.common.CallbackRequestHandler.handleCallbackRequest(CallbackRequestHandler.java:158)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.stubby.BigstoreStubbyImpl.listObjects(BigstoreStubbyImpl.java:821)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.common.ProxyDelegatorBase.callAndRecordLatency(ProxyDelegatorBase.java:109)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.stubby.StubbyProxyDelegator.call(StubbyProxyDelegator.java:184)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.api.stubby.BigstoreStubbyProxy.listObjects(BigstoreStubbyProxy.java:334)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.isolation.RpcReceiver.lambda$processRequestAsync$0(RpcReceiver.java:198)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.cloud.bigstore.isolation.AsyncExecutor.lambda$submit$0(AsyncExecutor.java:213)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.common.context.ContextRunnable.runInContext(ContextRunnable.java:83)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"io.grpc.Context.run(Context.java:536)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.tracing.GenericContextCallback.runInInheritedContext(GenericContextCallback.java:58)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"com.google.common.context.ContextRunnable.run(ContextRunnable.java:74)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\\\\\\\\\\\\" stack_entries: \\\\\\\\\\\\\"java.base/java.lang.Thread.run(Unknown Source)\\\\\\\\\\\\\" } } } [blobstore2.GcsErrorDetails] { xml_code: \\\\\\\\\\\\\"AccessDenied\\\\\\\\\\\\\" msg: \\\\\\\\\\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\\\\\\\\\" http_code: 403 details { first: \\\\\\\\\\\\\"Details\\\\\\\\\\\\\" second: \\\\\\\\\\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\\\\\\\\\" } debug_info: \\\\\\\\\\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\\\\\\\\\" } [cloud.bigstore.GcsLatencyInfo] { requests { method: \\\\\\\\\\\\\"/BackendObjectsService.List\\\\\\\\\\\\\" deadline { seconds: 29 nanos: 979569783 } start { seconds: 1752257982 nanos: 549935579 } end { seconds: 1752257982 nanos: 570470964 } status { code: 1 space: \\\\\\\\\\\\\"cloud.bigstore.ResponseCode.ErrorCode\\\\\\\\\\\\\" message: \\\\\\\\\\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\\\\\\\\\" canonical_code: 7 } } requests { method: \\\\\\\\\\\\\"/BucketMdService.LookupBucket\\\\\\\\\\\\\" deadline { seconds: 4 nanos: 999967650 } start { seconds: 1752257982 nanos: 551314439 } end { seconds: 1752257982 nanos: 553833108 } status { } metadata_spanner_stats { read_walltime_millis: 0 read_cpu_millis: 0 read_scheduler_delay_millis: 0 read_throttle_delay_millis: 0 read_per_service_limit_queue_delay_millis: 0 read_locking_delay_millis: 0 read_client_overhead_delay_millis: 0 read_client_flow_control_delay_millis: 0 read_io_delay_millis: 0 } elapsed_time_isolator_metrics { } sunspot_verdict: VERDICT_CAT_UNKNOWN } requests { method: \\\\\\\\\\\\\"/AccessService.CheckListObjects\\\\\\\\\\\\\" deadline { seconds: 29 nanos: 969642965 } start { seconds: 1752257982 nanos: 553970188 } end { seconds: 1752257982 nanos: 569602062 } status { } } }\\\\\"\\\\n  message_set {\\\\n    [google.rpc.error_details_ext] {\\\\n      message: \\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\"\\\\n    }\\\\n  }\\\\n}\\\\nerror_context {\\\\n  table_name: \\\\\"bigframes-dev._8b037bfb7316dddf9d92b12dcf93e008906bfe52.bqdf20250711_sessionee050e_bb3969eb60f548be8a4b6f5e2564f69f\\\\\"\\\\n}\\\\n\"\\ncause: USER_ERROR\\naddress: \"http://jfs5.prod.google.com:4901/task?handle=logs.2071.serving.shard-mals.cloud-dataengine.11785653463719 Partition description: __SHUFFLE0/0 TableDef \\\\\\'bigframes-dev._8b037bfb7316dddf9d92b12dcf93e008906bfe52.bqdf20250711_sessionee050e_bb3969eb60f548be8a4b6f5e2564f69f\\\\\\' of type \\\\\\'object-meta\\\\\\': /bigstore/your-bucket/audio-files/*\"\\nstatus_proto {\\n  code: 7\\n  space: \"generic\"\\n  message: \"Permission denied while globbing file pattern. bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\'storage.objects.list\\\\\\' denied on resource (or it may not exist). Please make sure gs://your-bucket/audio-files/* is accessible via appropriate IAM roles, e.g. Storage Object Viewer or Storage Object Creator.\"\\n}\\nerror_details {\\n  message: \"GCS aka Bigstore is not approved for storing Google user data (go/blobstore-gcs-getting-started). If you have obtained security exceptions for Bigstore instead of Blobstore, Please make sure Dremel has access to the files and all directories on the path to the files (http://go/dremel-access).\"\\n  permission_denied_error {\\n    accessed_resource_uri: \"/bigstore/your-bucket/audio-files/*\"\\n    system: BIGSTORE\\n  }\\n  underlying_status {\\n    code: 7\\n    space: \"generic\"\\n    message: \"Calling Match with file \\\\\"/bigstore/your-bucket/audio-files/**\\\\\": cloud.bigstore.ResponseCode.ErrorCode::ACCESS_DENIED: bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\'storage.objects.list\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\'storage.objects.list\\\\\\' denied on resource (or it may not exist). [google.rpc.error_details_ext] { message: \\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\" details { [type.googleapis.com/google.rpc.DebugInfo] { stack_entries: \\\\\"com.google.net.rpc3.client.RpcClientException: APPLICATION_ERROR;cloud.bigstore/FrontendObjectsService.List;bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).;AppErrorCode=1;StartTimeMs=1752257982548;tcp;Deadline(sec)=29.991333608;ResFormat=uncompressed;interceptors={[com.google.prod.fireaxe.filters.FireaxeRpcClientInterceptorImpl;com.google.cloud.bigstore.common.LatencyCollectingInterceptor;com.google.frameworks.debug.sherlog.core.rpcutil.Stubby3ClientInterceptor];overrides={}};ServerTimeSec=0.021769909;LogBytes=256;Non-FailFast;EffSecLevel=strong_privacy_and_integrity;ReqFormat=uncompressed;ReqID=f0298bdab819b7ef;GlobalID=632dd03b377a3319;Server=[::1]:14003\\\\\" stack_entries: \\\\\"com.google.net.rpc3.client.RpcStub.startBlockingRpcInternal(RpcStub.java:571)\\\\\" stack_entries: \\\\\"com.google.net.rpc3.client.RpcStub.startBlockingRpc(RpcStub.java:471)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.proto.BackendObjectsService$Stub.list(BackendObjectsService.java:734)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.common.LampreyServiceBase.call(LampreyServiceBase.java:37)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.common.BigstoreFrontendObjectsServiceImpl.call(BigstoreFrontendObjectsServiceImpl.java:40)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.common.BigstoreFrontendObjectsServiceImpl.list(BigstoreFrontendObjectsServiceImpl.java:97)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.stubby.BigstoreStubbyImpl.lambda$listObjects$0(BigstoreStubbyImpl.java:832)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.common.CallbackRequestHandler.handleCallbackRequest(CallbackRequestHandler.java:288)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.common.CallbackRequestHandler.handleCallbackRequest(CallbackRequestHandler.java:158)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.stubby.BigstoreStubbyImpl.listObjects(BigstoreStubbyImpl.java:821)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.common.ProxyDelegatorBase.callAndRecordLatency(ProxyDelegatorBase.java:109)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.stubby.StubbyProxyDelegator.call(StubbyProxyDelegator.java:184)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.api.stubby.BigstoreStubbyProxy.listObjects(BigstoreStubbyProxy.java:334)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.isolation.RpcReceiver.lambda$processRequestAsync$0(RpcReceiver.java:198)\\\\\" stack_entries: \\\\\"com.google.cloud.bigstore.isolation.AsyncExecutor.lambda$submit$0(AsyncExecutor.java:213)\\\\\" stack_entries: \\\\\"com.google.common.context.ContextRunnable.runInContext(ContextRunnable.java:83)\\\\\" stack_entries: \\\\\"io.grpc.Context.run(Context.java:536)\\\\\" stack_entries: \\\\\"com.google.tracing.GenericContextCallback.runInInheritedContext(GenericContextCallback.java:58)\\\\\" stack_entries: \\\\\"com.google.common.context.ContextRunnable.run(ContextRunnable.java:74)\\\\\" stack_entries: \\\\\"java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\\\\\" stack_entries: \\\\\"java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\\\\\" stack_entries: \\\\\"java.base/java.lang.Thread.run(Unknown Source)\\\\\" } } } [blobstore2.GcsErrorDetails] { xml_code: \\\\\"AccessDenied\\\\\" msg: \\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\" http_code: 403 details { first: \\\\\"Details\\\\\" second: \\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\" } debug_info: \\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\" } [cloud.bigstore.GcsLatencyInfo] { requests { method: \\\\\"/BackendObjectsService.List\\\\\" deadline { seconds: 29 nanos: 979569783 } start { seconds: 1752257982 nanos: 549935579 } end { seconds: 1752257982 nanos: 570470964 } status { code: 1 space: \\\\\"cloud.bigstore.ResponseCode.ErrorCode\\\\\" message: \\\\\"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist). bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\\\\\\\\\'storage.objects.list\\\\\\\\\\\\\\' denied on resource (or it may not exist).\\\\\" canonical_code: 7 } } requests { method: \\\\\"/BucketMdService.LookupBucket\\\\\" deadline { seconds: 4 nanos: 999967650 } start { seconds: 1752257982 nanos: 551314439 } end { seconds: 1752257982 nanos: 553833108 } status { } metadata_spanner_stats { read_walltime_millis: 0 read_cpu_millis: 0 read_scheduler_delay_millis: 0 read_throttle_delay_millis: 0 read_per_service_limit_queue_delay_millis: 0 read_locking_delay_millis: 0 read_client_overhead_delay_millis: 0 read_client_flow_control_delay_millis: 0 read_io_delay_millis: 0 } elapsed_time_isolator_metrics { } sunspot_verdict: VERDICT_CAT_UNKNOWN } requests { method: \\\\\"/AccessService.CheckListObjects\\\\\" deadline { seconds: 29 nanos: 969642965 } start { seconds: 1752257982 nanos: 553970188 } end { seconds: 1752257982 nanos: 569602062 } status { } } }\"\\n    message_set {\\n      [google.rpc.error_details_ext] {\\n        message: \"bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\'storage.objects.list\\\\\\' denied on resource (or it may not exist).\"\\n      }\\n    }\\n  }\\n  error_context {\\n    table_name: \"bigframes-dev._8b037bfb7316dddf9d92b12dcf93e008906bfe52.bqdf20250711_sessionee050e_bb3969eb60f548be8a4b6f5e2564f69f\"\\n  }\\n}\\n errorProto=code: \"ACCESS_DENIED\"\\nargument: \"BigQuery\"\\nargument: \"BigQuery\"\\nargument: \"Permission denied while globbing file pattern. bqcx-1084210331973-pcbl@gcp-sa-bigquery-condel.iam.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket. Permission \\\\\\'storage.objects.list\\\\\\' denied on resource (or it may not exist). Please make sure gs://your-bucket/audio-files/* is accessible via appropriate IAM roles, e.g. Storage Object Viewer or Storage Object Creator.\"\\nlocation_type: OTHER\\nlocation: \"gs://your-bucket/audio-files/*\"\\n\\n\\tat com.google.cloud.helix.common.Exceptions.fromProto(Exceptions.java:2016)\\n\\tat com.google.cloud.helix.common.dremel.QueryExecutorImpl.mapDremelErrorsTohelixException(QueryExecutorImpl.java:1194)\\n\\tat com.google.cloud.helix.common.dremel.QueryExecutorImpl$ConfiguredQueryMigration$StreamHandler.onMessage(QueryExecutorImpl.java:769)\\n\\tat com.google.cloud.helix.common.dremel.QueryExecutorImpl$ConfiguredQueryMigration$StreamHandler.onMessage(QueryExecutorImpl.java:695)\\n\\tat com.google.net.rpc3.stream.RpcMessageCallback$ForwardingRpcMessageCallback.onMessage(RpcMessageCallback.java:128)\\n\\tat com.google.net.rpc3.impl.RpcStreamInternalContext.processMessageUnlocked(RpcStreamInternalContext.java:1852)\\n\\tat com.google.net.rpc3.impl.RpcStreamInternalContext.invokeCallbacksInternalUnlocked(RpcStreamInternalContext.java:2904)\\n\\tat com.google.net.rpc3.impl.RpcStreamInternalContext.invokeCallbacksUnlocked(RpcStreamInternalContext.java:2830)\\n\\tat com.google.net.eventmanager.AbstractFutureTask$Sync.innerRun(AbstractFutureTask.java:259)\\n\\tat com.google.net.eventmanager.AbstractFutureTask.run(AbstractFutureTask.java:120)\\n\\tat com.google.net.eventmanager.EventManagerImpl.runTaskTraced(EventManagerImpl.java:900)\\n\\tat com.google.net.eventmanager.EventManagerImpl.runTask(EventManagerImpl.java:892)\\n\\tat com.google.net.eventmanager.EventManagerImpl.internalRunWorkerLoop(EventManagerImpl.java:1319)\\n\\tat com.google.net.eventmanager.EventManagerImpl.runWorkerLoop(EventManagerImpl.java:1210)\\n\\tat com.google.net.eventmanager.WorkerThreadInfo.runWorkerLoop(WorkerThreadInfo.java:153)\\n\\tat com.google.net.eventmanager.EventManagerImpl$WorkerThread.run(EventManagerImpl.java:1999)\\n'}]"
     ]
    }
   ],
   "source": [
    "import bigframes.pandas as bpd  \n",
    "import bigframes.enums  \n",
    "  \n",
    "# 1. Enable partial ordering mode  \n",
    "bpd.options.bigquery.ordering_mode = \"partial\"  \n",
    "  \n",
    "# 2. Create a DataFrame with blob data and null index  \n",
    "# Using from_glob_path which creates multimodal DataFrames  \n",
    "flattened = bpd.from_glob_path(  \n",
    "    \"gs://your-bucket/audio-files/*\",   \n",
    "    name=\"GCS Blob\"  \n",
    ")  \n",
    "  \n",
    "# Alternatively, create from URI strings with null index  \n",
    "# df = bpd.DataFrame({\"uri\": [\"gs://bucket/audio1.wav\", \"gs://bucket/audio2.wav\"]})  \n",
    "# df[\"GCS Blob\"] = df[\"uri\"].str.to_blob()  \n",
    "# flattened = bpd.read_gbq_table(\"your_table\", index_col=bigframes.enums.DefaultIndexKind.NULL)  \n",
    "  \n",
    "# 3. This will trigger the NullIndexError  \n",
    "flattened[\"Transcription\"] = flattened[\"GCS Blob\"].blob.audio_transcribe(  \n",
    "    model_name=\"gemini-2.0-flash-001\",  \n",
    "    verbose=True,  \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
