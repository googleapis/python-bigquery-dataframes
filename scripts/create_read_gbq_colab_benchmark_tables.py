# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from __future__ import annotations

import argparse
import base64
import datetime
import json
import math
from typing import Iterable, MutableSequence, Sequence

from google.cloud import bigquery
import numpy as np

# --- Input Data ---
# Generated by querying bigquery-magics usage. See internal issue b/420984164.
TABLE_STATS: dict[str, list[float]] = {
    "percentile": [9, 19, 29, 39, 49, 59, 69, 79, 89, 99],
    "materialized_or_scanned_bytes": [
        0.0,
        0.0,
        4102.0,
        76901.0,
        351693.0,
        500000.0,
        500000.0,
        1320930.0,
        17486432.0,
        1919625975.0,
    ],
    "num_materialized_or_scanned_rows": [
        0.0,
        6.0,
        100.0,
        4955.0,
        23108.0,
        139504.0,
        616341.0,
        3855698.0,
        83725698.0,
        5991998082.0,
    ],
    "avg_row_bytes": [
        0.00014346299635435792,
        0.005370969708923197,
        0.3692756731526246,
        4.079344721151818,
        7.5418,
        12.528863516404146,
        22.686258546389798,
        48.69689224091025,
        100.90817356205852,
        2020,
    ],
    "materialized_mb": [
        0.0,
        0.0,
        0.004102,
        0.076901,
        0.351693,
        0.5,
        0.5,
        1.32093,
        17.486432,
        1919.625975,
    ],
}

BIGQUERY_DATA_TYPE_SIZES = {
    "BOOL": 1,
    "DATE": 8,
    "FLOAT64": 8,
    "INT64": 8,
    "DATETIME": 8,
    "TIMESTAMP": 8,
    "TIME": 8,
    "NUMERIC": 16,
    # Flexible types.
    # JSON base size is its content, BYTES/STRING have 2 byte overhead + content
    "JSON": 0,
    "BYTES": 2,
    "STRING": 2,
}
FIXED_TYPES = [
    "BOOL",
    "INT64",
    "FLOAT64",
    "NUMERIC",
    "DATE",
    "DATETIME",
    "TIMESTAMP",
    "TIME",
]
FLEXIBLE_TYPES = ["STRING", "BYTES", "JSON"]

JSON_CHAR_LIST = list("abcdef")
STRING_CHAR_LIST = list("abcdefghijklmnopqrstuvwxyz0123456789")

# --- Helper Functions ---


def get_bq_schema(target_row_size_bytes: int) -> Sequence[tuple[str, str, int | None]]:
    """
    Determines the BigQuery table schema to match the target_row_size_bytes.
    Prioritizes fixed-size types for diversity, then uses flexible types.
    Returns a list of tuples: (column_name, type_name, length_for_flexible_type).
    Length is None for fixed-size types.
    """
    schema: MutableSequence[tuple[str, str, int | None]] = []
    current_size = 0
    col_idx = 0

    for bq_type in FIXED_TYPES:
        # For simplicity, we'll allow slight overage if only fixed fields are chosen.
        if current_size >= target_row_size_bytes:
            break

        type_size = BIGQUERY_DATA_TYPE_SIZES[bq_type]
        schema.append((f"col_{bq_type.lower()}_{col_idx}", bq_type, None))
        current_size += type_size
        col_idx += 1

    # Use flexible-size types to fill remaining space

    # Attempt to add one of each flexible type if space allows
    if current_size < target_row_size_bytes:
        remaining_bytes_for_content = target_row_size_bytes - current_size

        # For simplicity, divide the remaing bytes evenly across the flexible
        # columns.
        target_size = int(math.ceil(remaining_bytes_for_content / len(FLEXIBLE_TYPES)))

        for bq_type in FLEXIBLE_TYPES:
            base_cost = BIGQUERY_DATA_TYPE_SIZES[bq_type]
            min_content_size = max(0, target_size - base_cost)

            schema.append(
                (f"col_{bq_type.lower()}_{col_idx}", bq_type, min_content_size)
            )
            current_size += base_cost + min_content_size
            col_idx += 1

    return schema


def generate_bool_batch(
    num_rows: int, rng: np.random.Generator, content_length: int | None = None
) -> np.ndarray:
    return rng.choice([True, False], size=num_rows)


def generate_int64_batch(
    num_rows: int, rng: np.random.Generator, content_length: int | None = None
) -> np.ndarray:
    return rng.integers(-(10**18), 10**18, size=num_rows, dtype=np.int64)


def generate_float64_batch(
    num_rows: int, rng: np.random.Generator, content_length: int | None = None
) -> np.ndarray:
    return rng.random(size=num_rows) * 2 * 10**10 - 10**10


def generate_numeric_batch(
    num_rows: int, rng: np.random.Generator, content_length: int | None = None
) -> np.ndarray:
    raw_numerics = rng.random(size=num_rows) * 2 * 10**28 - 10**28
    format_numeric_vectorized = np.vectorize(lambda x: f"{x:.9f}")
    return format_numeric_vectorized(raw_numerics)


def generate_date_batch(
    num_rows: int, rng: np.random.Generator, content_length: int | None = None
) -> np.ndarray:
    start_date_ord = datetime.date(1, 1, 1).toordinal()
    max_days = (datetime.date(9999, 12, 31) - datetime.date(1, 1, 1)).days
    day_offsets = rng.integers(0, max_days + 1, size=num_rows)
    date_ordinals = start_date_ord + day_offsets
    return np.array(
        [
            datetime.date.fromordinal(int(ordinal)).isoformat()
            for ordinal in date_ordinals
        ]
    )


def generate_numpy_datetimes(num_rows: int, rng: np.random.Generator) -> np.ndarray:
    # Generate seconds from a broad range (e.g., year 1 to 9999)
    # Note: Python's datetime.timestamp() might be limited by system's C mktime.
    # For broader range with np.datetime64, it's usually fine.
    # Let's generate epoch seconds relative to Unix epoch for np.datetime64 compatibility
    min_epoch_seconds = int(
        datetime.datetime(1, 1, 1, 0, 0, 0, tzinfo=datetime.timezone.utc).timestamp()
    )
    # Max for datetime64[s] is far out, but let's bound it reasonably for BQ.
    max_epoch_seconds = int(
        datetime.datetime(
            9999, 12, 28, 23, 59, 59, tzinfo=datetime.timezone.utc
        ).timestamp()
    )

    epoch_seconds = rng.integers(
        min_epoch_seconds,
        max_epoch_seconds + 1,
        size=num_rows,
        dtype=np.int64,
    )
    microseconds_offset = rng.integers(0, 1000000, size=num_rows, dtype=np.int64)

    # Create datetime64[s] from epoch seconds and add microseconds as timedelta64[us]
    np_timestamps_s = epoch_seconds.astype("datetime64[s]")
    np_microseconds_td = microseconds_offset.astype("timedelta64[us]")
    return np_timestamps_s + np_microseconds_td


def generate_datetime_batch(
    num_rows: int, rng: np.random.Generator, content_length: int | None = None
) -> np.ndarray:
    np_datetimes = generate_numpy_datetimes(num_rows, rng)

    # np.datetime_as_string produces 'YYYY-MM-DDTHH:MM:SS.ffffff'
    # BQ DATETIME typically uses a space separator: 'YYYY-MM-DD HH:MM:SS.ffffff'
    datetime_strings = np.datetime_as_string(np_datetimes, unit="us")
    return np.array([s.replace("T", " ") for s in datetime_strings])


def generate_timestamp_batch(
    num_rows: int, rng: np.random.Generator, content_length: int | None = None
) -> np.ndarray:
    np_datetimes = generate_numpy_datetimes(num_rows, rng)

    # Convert to string with UTC timezone indicator
    # np.datetime_as_string with timezone='UTC' produces 'YYYY-MM-DDTHH:MM:SS.ffffffZ'
    # BigQuery generally accepts this for TIMESTAMP.
    return np.datetime_as_string(np_datetimes, unit="us", timezone="UTC")


def generate_time_batch(
    num_rows: int, rng: np.random.Generator, content_length: int | None = None
) -> np.ndarray:
    hours = rng.integers(0, 24, size=num_rows)
    minutes = rng.integers(0, 60, size=num_rows)
    seconds = rng.integers(0, 60, size=num_rows)
    microseconds = rng.integers(0, 1000000, size=num_rows)
    time_list = [
        datetime.time(hours[i], minutes[i], seconds[i], microseconds[i]).isoformat()
        for i in range(num_rows)
    ]
    return np.array(time_list)


def generate_json_row(content_length: int, rng: np.random.Generator) -> str:
    json_val_len = max(0, content_length - 5)
    json_val_chars = rng.choice(JSON_CHAR_LIST, size=json_val_len)
    json_obj = {"k": "".join(json_val_chars)}
    return json.dumps(json_obj)


def generate_json_batch(
    num_rows: int, rng: np.random.Generator, content_length: int | None = None
) -> np.ndarray:
    content_length = content_length if content_length is not None else 10
    json_list = [
        generate_json_row(content_length=content_length, rng=rng)
        for _ in range(num_rows)
    ]
    return np.array(json_list)


def generate_string_batch(
    num_rows: int, rng: np.random.Generator, content_length: int | None = None
) -> np.ndarray:
    content_length = content_length if content_length is not None else 1
    content_length = max(0, content_length)
    chars_array = rng.choice(STRING_CHAR_LIST, size=(num_rows, content_length))
    return np.array(["".join(row_chars) for row_chars in chars_array])


def generate_bytes_batch(
    num_rows: int, rng: np.random.Generator, content_length: int | None = None
) -> np.ndarray:
    content_length = content_length if content_length is not None else 1
    content_length = max(0, content_length)
    return np.array(
        [
            base64.b64encode(rng.bytes(content_length)).decode("utf-8")
            for _ in range(num_rows)
        ]
    )


BIGQUERY_DATA_TYPE_GENERATORS = {
    "BOOL": generate_bool_batch,
    "DATE": generate_date_batch,
    "FLOAT64": generate_float64_batch,
    "INT64": generate_int64_batch,
    "DATETIME": generate_datetime_batch,
    "TIMESTAMP": generate_timestamp_batch,
    "TIME": generate_time_batch,
    "NUMERIC": generate_numeric_batch,
    "JSON": generate_json_batch,
    "BYTES": generate_bytes_batch,
    "STRING": generate_string_batch,
}


def generate_random_data(
    schema: Sequence[tuple[str, str, int | None]],
    num_rows: int,
    rng: np.random.Generator,
    batch_size: int,
) -> Iterable[list[dict]]:
    """
    Generates random data for the given schema and number of rows, yielding batches.
    """
    if num_rows == 0:
        yield []
        return

    col_names_ordered = [s[0] for s in schema]

    generated_rows_total = 0
    while generated_rows_total < num_rows:
        current_batch_size = min(batch_size, num_rows - generated_rows_total)
        if current_batch_size == 0:
            break

        columns_data_batch = {}
        for col_name, bq_type, length in schema:
            generate_batch = BIGQUERY_DATA_TYPE_GENERATORS[bq_type]
            columns_data_batch[col_name] = generate_batch(
                current_batch_size, rng, content_length=length
            )

        # Turn numpy objects into Python objects.
        # https://stackoverflow.com/a/32850511/101923
        columns_data_batch_json = {}
        for column in columns_data_batch:
            columns_data_batch_json[column] = columns_data_batch[column].tolist()

        # Assemble batch of rows
        batch_data = []
        for i in range(current_batch_size):
            row = {
                col_name: columns_data_batch_json[col_name][i]
                for col_name in col_names_ordered
            }
            batch_data.append(row)

        yield batch_data
        generated_rows_total += current_batch_size


def create_and_load_table(
    client: bigquery.Client | None,
    project_id: str,
    dataset_id: str,
    table_name: str,
    schema_def: Sequence[tuple[str, str, int | None]],
    num_rows: int,
    rng: np.random.Generator,
):
    """Creates a BigQuery table and loads data into it by consuming a data generator."""

    # BQ client library streaming insert batch size (rows per API call)
    # This is different from data_gen_batch_size which is for generating data.
    # We can make BQ_LOAD_BATCH_SIZE smaller than data_gen_batch_size if needed.
    BQ_LOAD_BATCH_SIZE = 500

    # Actual BigQuery operations occur here because both project_id and dataset_id are provided
    print(
        f"Attempting BigQuery operations for table {table_name} in project '{project_id}', dataset '{dataset_id}'."
    )
    table_id = f"{project_id}.{dataset_id}.{table_name}"

    bq_schema = []
    for col_name, type_name, _ in schema_def:
        bq_schema.append(bigquery.SchemaField(col_name, type_name))

    table = bigquery.Table(table_id, schema=bq_schema)

    if client:
        print(f"(Re)creating table {table_id}...")
        table = client.create_table(table, exists_ok=True)
        print(f"Table {table_id} created successfully or already exists.")

        # Query in case there's something in the streaming buffer already.
        table_rows = next(
            iter(client.query_and_wait(f"SELECT COUNT(*) FROM `{table_id}`"))
        )[0]
        print(f"Table {table_id} has {table_rows} rows.")
        num_rows = max(0, num_rows - table_rows)

    else:
        print(f"Simulating: Generated schema: {schema_def}")

    if num_rows <= 0:
        print(f"No rows to load. Requested {num_rows} rows. Skipping.")
        return

    print(f"Starting to load {num_rows} rows into {table_id} in batches...")
    total_rows_loaded = 0

    batch_num_gen = 0
    for generated_data_chunk in generate_random_data(
        schema_def, num_rows, rng, BQ_LOAD_BATCH_SIZE
    ):
        batch_num_gen += 1
        if not generated_data_chunk:
            continue

        if batch_num_gen == 1 or batch_num_gen % 10 == 0:
            print(
                f"  Processing generated chunk {batch_num_gen} (size: {len(generated_data_chunk)} rows)..."
            )

        if not client:
            num_simulated_batches = math.ceil(num_rows / BQ_LOAD_BATCH_SIZE)
            print(
                f"Simulating: Would generate and load approx. {num_simulated_batches} batches."
            )
            print(f"Sample row: {generated_data_chunk[0]}")
            break

        errors = client.insert_rows_json(table, generated_data_chunk)
        if errors:
            raise ValueError(f"Encountered errors while inserting sub-batch: {errors}")

        total_rows_loaded += len(generated_data_chunk)


# --- Main Script Logic ---
def main():
    """Main function to create and populate BigQuery tables."""

    parser = argparse.ArgumentParser(
        description="Generate and load BigQuery benchmark tables."
    )
    parser.add_argument(
        "-p",
        "--project_id",
        type=str,
        default=None,
        help="Google Cloud Project ID. If not provided, script runs in simulation mode.",
    )
    parser.add_argument(
        "-d",
        "--dataset_id",
        type=str,
        default=None,
        help="BigQuery Dataset ID within the project. If not provided, script runs in simulation mode.",
    )
    args = parser.parse_args()

    rng = np.random.default_rng(seed=42)  # Seed for reproducibility
    num_percentiles = len(TABLE_STATS["percentile"])
    client = None

    if args.project_id and args.dataset_id:
        client = bigquery.Client(project=args.project_id)
        dataset = bigquery.Dataset(f"{args.project_id}.{args.dataset_id}")
        client.create_dataset(dataset, exists_ok=True)

    for i in range(num_percentiles):
        percentile = TABLE_STATS["percentile"][i]
        avg_row_bytes_raw = TABLE_STATS["avg_row_bytes"][i]
        num_rows_raw = TABLE_STATS["num_materialized_or_scanned_rows"][i]

        target_row_bytes = max(1, int(math.ceil(avg_row_bytes_raw)))
        num_rows = max(1, int(math.ceil(num_rows_raw)))

        table_name = f"percentile_{percentile:02d}"
        print(f"\n--- Processing Table: {table_name} ---")
        print(f"Target average row bytes (rounded up): {target_row_bytes}")
        print(f"Number of rows (rounded up): {num_rows}")

        schema_definition = get_bq_schema(target_row_bytes)
        print(f"Generated Schema: {schema_definition}")

        create_and_load_table(
            client,
            args.project_id or "",
            args.dataset_id or "",
            table_name,
            schema_definition,
            num_rows,
            rng,
        )


if __name__ == "__main__":
    main()
